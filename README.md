
# VTF-Net: A Visual Temporal Feature Network for Robust Retinal OCT Image Segmentation
### [Project page](https://github.com/IMOP-lab/VTF-Net-Pytorch) | [Our laboratory page](https://github.com/IMOP-lab)
by Xingru Huang, Zhengyao Jiang, Zhao Huang, Yihao Guo, Jian Huang, Changpeng Yue, Jin Liu, Zhiwen Zheng, Xiaoshuai Zhang

Hangzhou Dianzi University IMOP-lab
![Figure1：Detailed network structure of our proposed VTF-NET](https://github.com/IMOP-lab/VTF-Net-Pytorch/blob/main/figures/Fig2.png)
Figure1: Detailed network structure of our proposed VTF-NET

The framework initiates with five feature maps $\{f_1, f_2, f_3, f_4, f_5\}$ generated by the ResNet backbone, successively processed through distinct components. The first four maps undergo transformation within four layers of the VTFE module, yielding enhanced outputs $\{f_1', f_2', f_3', f_4'\}$. Simultaneously, the fifth feature map $f_5$ is refined by the MSAF module, producing $f_5'$. These outputs collectively encapsulate spatiotemporal attributes and multi-resolution information, which align with the specific roles of individual network modules. The AFRP extracts and synchronizes inter-feature dependencies via RSR and ADU, optimizing temporal coherence and feature fidelity. Subsequently, the refined features pass through the EFRE, which reconstructs the final predictions, minimizing boundary ambiguity. Parameter updates occur iteratively through loss computation, ensuring optimized segmentation output with each cycle.

## Methods

### VTFE
![Fugure2](https://github.com/IMOP-lab/VTF-Net-Pytorch/blob/main/figures/Fig3.png)
Figure2: Visual representation of the VTFE structure. 

The input feature maps, generated by ResNet34, undergo progressive transformations as they traverse multiple layers. These transformations begin with the flattening of multi-dimensional tensors into matrices, facilitating the subsequent linear mapping. Afterward, the features are processed through cascades of convolutional operations, batch normalization, and ReLU activation, enhancing representational depth. Temporal dynamics are captured via xLSTM modules, which integrate spatial-temporal correlations, further augmented by residual connections that safeguard information fidelity. 
This process preserves both fine-grained and global structural details.

### MSAF
![Fugure3](https://github.com/IMOP-lab/VTF-Net-Pytorch/blob/main/figures/Fig4.png)
Figure3: Struture of MSAF 

The MSAF module structure decomposes input data into three streams to facilitate enhanced feature extraction across both spatial and frequency domains. The first stream preserves original feature integrity through skip connections, retaining essential structural information. The second stream employs channel partitioning, followed by Fast Fourier Transform to independently extract high and low frequency components, thereby augmenting spectral representation. Simultaneously, the third stream utilizes convolutional interpolation and pooling operations to capture directional features, contributing to the model’s spatial sensitivity. After individual processing, these streams undergo multi-scale fusion and are integrated with the original input features through convolutional operations, generating output tensors aligned to the desired dimensionality.


## Installation
The hardware configuration consisted of a desktop system equipped with two NVIDIA 3080 GPUs, an Intel E5-2690V4 CPU, and 256 GB of RAM. The software environment was constituted of Python 3.9, PyTorch 2.0.0, and CUDA 11.8, with the training framework being realized through PyTorch's DistributedDataParallel (DDP) implementation.

## Experiment

### Datasets
|Datasets	| Quantity |  Training Set |	Validation Set | Testing Set|
|-|-|-|-|-|
|CMED-18k|10000|7200|800|2000|

### baseline
We provide GitHub links pointing to the PyTorch implementation code for all networks compared in this experiment here, so you can easily reproduce all these projects.

[UNet](https://github.com/milesial/Pytorch-UNet);[FCN8s](); [SegNet](); [PSPNet](); [ENet](); [ICNet](); [UNet+AttGate]() [DANet](); [LEDNet](); [DUNet](); [CENet](); [CGNet](); [OCNet](); [GCN](), 
### Results
![Table1](https://github.com/IMOP-lab/VTF-Net-Pytorch/blob/main/figures/Table1.jpg)
Table1: The results of segmentation performance of the proposed method against 14 baseline models, evaluated on the CMED-18K dataset. Metrics include dice coefficient, HD, HD95, NCC, and Kappa statistic. The highest performance values for each metric are highlighted in red, with the second highest marked in blue.

![Figure2](https://github.com/IMOP-lab/VTF-Net-Pytorch/blob/main/figures/Fig5.png)
Figure4: Illustration of results between VTF-Net and 14 baseline models. The first row presents the original input images, followed by corresponding results, including zoomed-in views of edema regions to highlight segmentation detail.

All experiments were executed under identical conditions, and the results are detailed in Table1 and Figure2. VTF-Net showed competitive results across various evaluation metrics.

## Abaltion study

### Key components of VTFE

![Table2](https://github.com/IMOP-lab/VTF-Net-Pytorch/blob/main/figures/Table2.jpg)
Table2: Ablation study results for the VTF-Net architecture, comparing the impact of individual modules—VTFE, MSAF, AFRP, and EFRE—on segmentation performance across multiple metrics, including Dice coefficient, HD, HD95, NCC, and Kappa. The highest performance values are highlighted in red, while the second-highest are marked in blue, demonstrating the relative contributions of each module to the overall network efficacy.

![Table3](https://github.com/IMOP-lab/VTF-Net-Pytorch/blob/main/figures/Table3.jpg)
Table3: Ablation study results for various attention fusion strategies within the MSAF module, illustrating their differential impacts on segmentation performance across multiple quantitative metrics. The CA and EMA attention mechanisms represent Coordinate Attention and Efficient Multi-Scale Attention, respectively, while CB denotes Convolutional Block Attention Module. FFT refers to the Fast Fourier Transform, LSK indicates a Large Selective Kernel Network, and CA EMA signifies the serial concatenation of CA and EMA outputs. The configuration labeled FFT + CA EMA demonstrates a parallel fusion of FFT and CA EMA outputs, and FSA represents a frequency split attention strategy. Red text highlights the highest values, whereas the second-highest scores are marked in blue, signifying performance optima across configurations.

![Table4](https://github.com/IMOP-lab/VTF-Net-Pytorch/blob/main/figures/Table4.jpg)
Table4: Detailed results conducted on the VTFE module, evaluating the impact of variations in architectural parameters number of layers, kernel sizes, and convolutional blocks on segmentation metrics. Multiple configurations were tested to determine the optimal combination of these parameters, with the highest metric values marked in red and the second-highest in blue. The configuration employing 4 layers, 5x5 kernels, and 2 convolutional blocks demonstrated the most favorable performance, indicating the importance of deeper feature hierarchies and larger receptive fields for capturing complex patterns in retinal OCT segmentation.

## Question
If you have any question, please concat 'zhengyao.jiang@hdu.edu.cn.
